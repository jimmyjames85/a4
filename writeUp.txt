Homework #4
Jim Tappe

======================================================================

Problem 1

Part A:
First let us consider the best and worst case running times for the
function add() Since this is a recursive funtion the base case is when
add attempts to add to the left but left is empty or when add attempts
to add to the right but right is empty. 

public void add(int toAdd)
{
	if (toAdd < data)
	{
		if (left == null)
			left = new BST(toAdd);
		else
			left.add(toAdd);
	}
	else
	{
		if (right == null)
			right = new BST(toAdd);
		else
			right.add(toAdd);
	}
}

Let us consider the unbalanced tree

  1
    2
      3
        4
          5


and suppose we call add(6) on the root (1). We see 
6 is greater than (1) so we call right.add(toAdd).
6 is greater than (2) so we call right.add(toAdd).
6 is greater than (3) so we call right.add(toAdd).
6 is greater than (4) so we call right.add(toAdd).

6 is greater than (5) but (5) has no child so we create a new BST. All
in all we've called the function add() 5 times. 5 is the number of
elements in the tree thus the worst case scenario for add() is when
the tree is unbalanced and runs in n time.


Part B:
Now consider a well-ballanced binary tree

                       8
              4                   12
        2         6         10         14
      1   3      5  7         11     13  15

and suppose we call add(9) on the root (8). We see
9 is greater than (8) so we call right.add(toAdd). 
9 is smaller than (12) so we call left.add(toAdd).
9 is smaller than (10) but (10) has no left child so we create a new
BST. All in all we've called the function add() 3 times. 3 is is equal
to lg(n) in the integer world and thus thus the best case scenario is
when the tree is well-balanced and runs in lg(n)-time.

If we look at add() as a function on the number of elements in
the tree, f(n), we see add will run in f(n) time where:

       lg(n) ≤ f(n) ≤ n 

Thus add() ∊ BigO(n)  and add() ∊ Omega(lg(n))

Now consider the Build-BST(list) algorithm

public static BST buildBst(int[] list)
{
	if(list.length < 0)                      
	       return null;

	BST root = new BST(list[0]);

	for (int i = 1; i < list.length; i++)
		root.add(list[i]);

	return root;
}

We are mainly concerned with the for loop adding n elements from the
list. Every thing else runs in constant time. Thus the loop calls
root.add() n times which runs in f(n) time. Thus BuildBST will run in
n·f(n). But f(n) ∊ BigO(n) and f(n) ∊ BigOmega(lg(n)) thus n·f(n) ∊
BigO(n²) and n·f(n) ∊ Omega(n·lg(n)). In other words the best case
running time of BuildBST is BigOmega(n·lg(n)) and the worst case
running time of BuildBST is BigO(n²).


Part C:

An AVL-tree is a self-balancing tree. If Build-BST self-balanced or
maintained a height of lg(n) after every insert it would reduce the
worst-case scenario down to the best case scenario. Thus Build-BST
would run in Theta(n*lg(n)) amortized time. I say amortized because of
the cases where an add renders the BST unbalanced, in which case we
must factor in the runtime of rebalancing the tree. With an AVL-tree
add is BigO(lg(n)) amortized time. Thus our for loop adds n items and
runs in ThetaO(n*lg(n)).

======================================================================

Problem 2

We want to merge k sorted lists with a total of n elements into one
sorted list. To do this we insert each sorted list into a min-heap
where the value that is used for comparison is the smallest value in
each sorted list. We can do this in BigO(k) time. Next we simply 'pop'
off the next element of the top list. Everytime we pop we must remove
that element of the top list, and percolate the list down the tree,
for it may not be the smallest element. Percolate will take
BigO(lg(k)) since there are k lists. Since we must first build the
list and since we have a total of n elements and for each element we
must percolate down lg(k) times, the total running time will be BigO(k
+ n*lg(k)) = BigO(n*lg(k)).


percolateDown(arr[], int elem)
{
	while(arr[elem] is larger than arr[smallestChild])
	{
	    swap (arr[elem],arr[smallestChild])	
	    //smallestChild index now has the current elem
	    percolateDown(arr,smallestChild);
	}
}

Because the smallestChild is either 2n+1 or 2n+2, the worst case
scenario in which we must percolate all the way down the tree, we must
call the function at most lg(arr.size) times. That is the initial call
to percolateDown was on the root of the tree (elem=0) and the element
being percolated down must travel down the entire height of the
tree. The best case scenario is if either the elem is smaller than
both its children, or has no children, in which case this function is
called only once. Thus percolateDown runs in BigO(lg(arr.size)).



//Suppose SortedList is an array of sorted comparable elements with
//the smallest element at index 0 and arr is an array of SortedList's
heapify(arr[])
{
	//Start at the last internal node percolate down until we
	//reach the root

	int curNode = (arr.size())/2;
	while(curNode>=0)
		percolateDown(curNode--);
}


This function relies on percolateDown() which we know to be
BigO(lg(arr.size)). But we only call percolateDown on non-leaf nodes,
thus we halved our problem size from the start. Consider the following
tree

                                    A 
                                   / \ 
                                 /     \ 
                               /         \
                              B           C
                            /   \       /   \
                           D     E     F     G
                          /\    / \   / \   / \
                         o  o  o   o o   o o   o

We only must call percolateDown on nodes A through G. Now a call to
percolateDown on D through G will in the worst case require one swap,
whereas a call to percolateDown on B or C will in the worst case
require at most two swaps. The worstcase scenario is avoided until we
call percolateDown on the root node A. Thus the worst case scenario
(in which each node A through G is percolated all the way down) is
T(k) = 1*(k/2) + 2*(k/4) + 3*(k/8) + ... + (k/2)*(1). I claim that
T(k) <= k.

See Proof1.jpg

Thus T(k)= BigO(k) and heapify runs in BigO(k). Once we have setup our
min-heap with the k sorted lists, our merge becomes quite simple.

arr[] merge(SortedList[] heap)
{
   int i=0;
   while(!heap.isEmpty)
   {
      arr[i]=heap[0].pop();
      percolateDown(heap,0);
      if the sortedList at heap[0] is empty remove it;
   }

   return arr;
}

We know there are n elements total in the k sorted lists. Thus the
while loop iterates n times. Each iteration calls percolateDown which
we know runs in BigO(lg(k)) time since there are k sorted lists. Thus
merge runs in BigO(n*lg(k)) time.

======================================================================

Problem 3.

Given a list of n elements we want to return the kth smallest
element. Again we use the method above to heapify the list. We know
this will take BigO(n) time. Then we want to simply pop off k-1
elements and return the kth element which will take k*lg(n) time. In
total the method will run in BigO(n+k*lg(n)) = BigO(lg(n)).


int findKthSmallestElem(arr[], int k)
{
	heapify(arr); 
	for(int i=0;i<k-1;i++)
	{
	
	   //swap the last element in the arr with the 0th element
	   swap(arr,0,arr.size-1);
	   //remove the last element which is now the smallest element	   
	   remove(arr,arr.size-1);
	   //maintain heap property
	   percolateDown(arr,0);
	}

	//we've removed k-1 smallest elements
	//now return the kth smallest element
	return arr[0];
}


The method heapify runs in BigO(n). The for loop iterates k times,
eachtime calling percolateDown thus running in BigO((k-1)*lg(n))
time. Thus findKthSmallestElem runs in BigO(n) + BigO(k*lg(n)). If k
is a constant and is relatively small then this method runs in BigO(n)
time. BUT if k is a variable passed to findKthSmallestElem then the
worst case is where k=n. Thus the method would run in BigO(n) +
BigO(n*lg(n)) = BigO(n*lg(n)).

======================================================================

Problem 4

T(n) = {  1              n=1
          7*T(n/2)+1     n>1

a) Thm: T(n)=ThetaO(n^(log2(7)))
    Pf: By master theorem
    	
	We have a=7,b=2,f(n)=1, n^logb(a)=n^log2(7)

	We see that most definately f(n)=1 is in Theta(n^( log2(7)-e))
	where e=0.8. Thus by the master thereom we can conclude that
	T(n) is in Theta(n^log2(7))

b) Thm: T(n) = 1/6(7n^log2(7)  - 1)
   See Proof2.jpg

======================================================================

Problem 5

H is a hashtable with 9 bins. The hash function is f(n)=n. We insert
the following sequence of integers and show the results: 
         0,39,7,12,42,4,17,13,3

Step 1.   0(mod 9) = 0
Step 2.  39(mod 9) = 3
Step 3.   7(mod 9) = 7
Step 4.  12(mod 9) = 3
Step 5.  42(mod 9) = 6
Step 6.   4(mod 9) = 4
Step 7.  17(mod 9) = 8
Step 8.  13(mod 9) = 4
Step 9.   3(mod 9) = 3


Step:  1    2    3    4    5    6    7    8    9
H[0]:  0
H[1]: 
H[2]:
H[3]:	    39        12                       3
H[4]:                           4         13
H[5]:
H[6]:                      42        
H[7]:		 7
H[8]:                                17

H[0]:0
H[1]: 
H[2]:
H[3]:39,12,3
H[4]:4,13
H[5]:
H[6]:42
H[7]:7
H[8]:17

======================================================================

Problem 6

A contracting array-list has the constraint that the capacity of the
array-list never exceeds twice its logical size.



add(item)
{
    if(logicalSize==arr.size)
	doubleArrayCapacity();
    
    arr[logicalSize++]=item;
}


doubleArrayCapacity()
{
    newArr[] = new Array[2*arr.size];
    for(int i=0;i<arr.size;i++)
    	newArr[i]=arr[i];

    delete arr;
    arr=newArr;
}


doubleArrayCapacity clearly runs in BigO(n) since its for loop must
copy over n elements to the new array. However, add only calls this
method when n is a power of two and thus runs in amortized constant
time. Suppose that arr is full thus arr.length is a power of two. When
we add an item we first doubleArrayCapacity which will run in BigO(n)
time. But since we have doubled our list we know have n empty spots in
our array to add to. Since these spots are empty we know have n adds
available to us that will take BigO(1) time. Thus on average we have
(n*BigO(1)+BigO(n))/n = BigO(1)+BigO(1) which runs in amortized
constant time.


item remove()
{
    item ret = arr[logicalSize--];
    if(logicalSize<arr.size/2)
       halfArrayCapacity();

    return ret;
}


halfArrayCapacity()
{
   newArr[]=new Array[arr.size/2];
   for(int i=0;i<newArr.length;i++)
       newArr[i]=arr[i];

   delete arr;
   arr = newArr;
}

Again halfArrayCapacity clearly runs in BigO(n/2). But remove only
calls this methods when n/2 is a power of two. Suppose that
logicalSize==arr.size/2 + 1. We remove the last item and now the
capicity exceeds twice the logical size and we must call
halfArrayCapacity which runs in BigO(n) time. However by resizing,
this gives n/2 items that we can remove in constant time. Thus remove
runs in [(n/2)*BigO(1)+BigO(n/2)]/(n/2) = BigO(1) + BigO(1) which runs
in amortized constant time
